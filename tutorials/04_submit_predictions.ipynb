{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Predictions\n",
    "\n",
    "This notebook shows how to prepare a submission of your model's predictions on the test data for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "from safetensors.torch import load_model\n",
    "import torch \n",
    "import pandas as pd\n",
    "\n",
    "from utils import hvatnet\n",
    "from utils.creating_dataset import LEFT_TO_RIGHT_HAND\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "\n",
    "This code loads the pre-trained baseline model - might be different for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "dtype = torch.float32\n",
    "\n",
    "weights = r\"C:\\Users\\feder\\Documents\\github\\BCI_ALVI_challenge\\tutorials\\logs\\test_2_run_fedya\\step_3300_loss_0.2750.safetensors\"\n",
    "\n",
    "MODEL_TYPE = 'hvatnet'\n",
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "\n",
    "load_model(model, weights)\n",
    "\n",
    "model = model.to(device).to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `submission.cvs` file\n",
    "\n",
    "This code shows how the data was prepare and downsampled during inference. Make sure that your data is processed similarly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(r\"F:\\Dropbox (Personal)\\BCII\\BCI Challenges\\2024 ALVI EMG Decoding\\dataset_v2_blocks\\dataset_v2_blocks\")\n",
    "test_data_name = 'fedya_tropin_standart_elbow_left'  # shoould match `test_dataset_list` used to train the model\n",
    "\n",
    "\n",
    "data_folder = DATA_PATH / \"amputant\" / \"left\" / test_data_name / \"preproc_angles\" / \"submit\"\n",
    "all_paths = natsorted(data_folder.glob('*.npz'))\n",
    "print(f'Found {len(all_paths)} samples in {data_folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_list = []\n",
    "\n",
    "# loop over each trial\n",
    "for i, p in enumerate(all_paths):\n",
    "    # get EMG data \n",
    "    sample = np.load(p)\n",
    "    myo = sample['data_myo']\n",
    "    myo = myo[:, LEFT_TO_RIGHT_HAND]\n",
    "\n",
    "    # predictions will have to be downsampled\n",
    "    gt_len = myo[::8].shape[0]\n",
    "\n",
    "    # padding\n",
    "    target_length = (myo.shape[0] + 255) // 256 * 256\n",
    "    padded_myo = np.pad(myo, ((0, target_length - myo.shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "    # some prediction. might be slididng window.\n",
    "    preds = model.inference(padded_myo)\n",
    "    preds_downsampled = preds[:gt_len]\n",
    "    print(f\"Completed {i+1}/{len(all_paths)}. Loaded data: {myo.shape} - padded to: {padded_myo.shape} - predictions {preds.shape} - downsampled to: {preds_downsampled.shape}\")\n",
    "    pred_list.append(preds_downsampled)\n",
    "\n",
    "pred_cat = np.concatenate(pred_list, axis=0)\n",
    "df = pd.DataFrame(pred_cat)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the predictions, your data should also include a sample id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, \"sample_id\", range(1, 1 + len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save to a CSV file. This is what you'll upload to Kaggle for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submit_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
