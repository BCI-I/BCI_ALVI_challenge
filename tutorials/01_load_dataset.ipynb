{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "from utils.augmentations import get_default_transform\n",
    "from utils import creating_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading & inspection\n",
    "This notebook shows how to load the datasets used in this challenge and provides some basic statistics about them.\n",
    "\n",
    "Note that the data includes EMG signal from 8 electrodes in the EMG armband, the predicted variables are the angle of 20 joints in the hand. Inputs are sampled at 200Hz, but the  outputs are intended to be at a subsampled rate of 25Hz (more on that in other notebooks).\n",
    "\n",
    "Data was acquired in \"healthy\" and \"amputant\" subjects (i.e. with limb loss) using the EMG armband in either the left or right arm. \n",
    "Your model's predictions will be evaluated on data from one of the two amputant subjects (fedya), but you can use any and all provided data for training your model. For simplicity, the data has been divided in `training` and `test` subsets (not all subjects have `test` data) to evaluate your model's performance during training. The final submission data is held separate and only used when preparing the submission.csv file (see 04_submit_predictions.ipynb).\n",
    "\n",
    "### Load data\n",
    "Start by defining a variable to keep track of where the data is saved on your computer and a set of parameters for selecting which data to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"F:\\Dropbox (Personal)\\BCII\\BCI Challenges\\2024 ALVI EMG Decoding\\dataset_v2_blocks\\dataset_v2_blocks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can decide which data to load (e.g. from both `health` and `amputant` subjects). \n",
    "The `test_dataset_list` specifies which subset of the data should be used as test set, and it shouldn't change. You can, however, play around with training your model on different subsets of the available training data. \n",
    "\n",
    "You can also define a set of `transform` functions to apply to the data before feeding it to the model, or load the default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = dict(\n",
    "    datasets=[DATA_PATH],\n",
    "    hand_type = ['left', 'right'], # [left, 'right']\n",
    "    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "    test_dataset_list = ['fedya_tropin_standart_elbow_left']  # don't change this !\n",
    ")\n",
    "\n",
    "# define a config object to keep track of data variables\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "\n",
    "# get transforms\n",
    "p_transform = 0.1  # probability of applying the transform\n",
    "transform = get_default_transform(p_transform)\n",
    "\n",
    "# load the data\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "`train_dataset` and `test_dataset` are instances of the `torch.utils.data.ConcatDataset` class. \n",
    "\n",
    "\n",
    "The following code shows the number of batches in each set as well as the size of input and outputs. \n",
    "Note that inputs are of shape `n_channels x batch_size` while the outputs are of shape `n_angles x downsampled_batchs_size` where `downsampled_batch_size = batch_size / 200 * 25` to account for downsampling of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}, Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a video of the hand movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hand_visualize import Hand, save_animation\n",
    "from utils.quats_and_angles import get_quats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batches = [train_dataset[i] for i in range(10)]\n",
    "Y = np.concatenate([b[1] for b in batches], axis=1)\n",
    "quats = get_quats(Y)\n",
    "\n",
    "hand_gt = Hand(quats)\n",
    "ani = hand_gt.visualize_all_frames()\n",
    "save_animation(ani, 'test_vis.gif', fps=25,)   # this will save a .gif file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
